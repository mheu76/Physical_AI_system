"""
PHI-3.5 ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ë” ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ PHI-3.5 ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import sys
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_phi35_simple():
    """ê°„ë‹¨í•œ PHI-3.5 í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸš€ PHI-3.5 ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        import transformers
        logger.info(f"âœ… Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°œê²¬: {transformers.__version__}")
        
        # ìºì‹œ ì •ë¦¬
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("âœ… CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        # PHI-3.5 ëª¨ë¸ ë¡œë”© ì‹œë„
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        model_name = "microsoft/Phi-3.5-mini-instruct"
        logger.info(f"ğŸ”„ PHI-3.5 ëª¨ë¸ ë¡œë”© ì‹œë„: {model_name}")
        
        # ë””ë°”ì´ìŠ¤ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë”© (CPU ëª¨ë“œë¡œ ì‹œë„)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # ìºì‹œ ë¹„í™œì„±í™” ì„¤ì •
        model.config.use_cache = False
        logger.info("âœ… ìºì‹œ ë¹„í™œì„±í™” ì„¤ì • ì™„ë£Œ")
        
        logger.info("âœ… PHI-3.5 ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        prompt = "Hello, how are you?"
        logger.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        logger.info("ğŸ”„ í…ìŠ¤íŠ¸ ìƒì„± ì‹œë„...")
        
        # ë§¤ìš° ê°„ë‹¨í•œ ìƒì„± ì‹œë„ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)
        try:
            with torch.no_grad():
                # ëª¨ë¸ì„ CPUë¡œ ì´ë™
                model = model.to('cpu')
                inputs = {k: v.to('cpu') for k, v in inputs.items()}
                
                # ìµœì†Œí•œì˜ íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±
                outputs = model.generate(
                    inputs["input_ids"],
                    max_new_tokens=50,
                    do_sample=False,  # ê²°ì •ì  ìƒì„±
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=False  # ìºì‹œ ì‚¬ìš© ì•ˆí•¨
                )
            
            # ë””ì½”ë”©
            generated_text = tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
            
            logger.info(f"âœ… ìƒì„±ëœ í…ìŠ¤íŠ¸: {generated_text}")
            
        except AttributeError as e:
            if "seen_tokens" in str(e):
                logger.warning("âš ï¸ DynamicCache ë¬¸ì œ ê°ì§€, ëŒ€ì•ˆ ë°©ì‹ ì‹œë„...")
                
                # ëŒ€ì•ˆ ë°©ì‹: ì§ì ‘ forward í˜¸ì¶œ
                with torch.no_grad():
                    model = model.to('cpu')
                    inputs = {k: v.to('cpu') for k, v in inputs.items()}
                    
                    # ì§ì ‘ forward í˜¸ì¶œ
                    outputs = model(**inputs)
                    logits = outputs.logits
                    
                    # ê°„ë‹¨í•œ ë‹¤ìŒ í† í° ì˜ˆì¸¡
                    next_token = torch.argmax(logits[0, -1, :])
                    generated_text = tokenizer.decode([next_token.item()], skip_special_tokens=True)
                    
                    logger.info(f"âœ… ëŒ€ì•ˆ ë°©ì‹ìœ¼ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸: {generated_text}")
            else:
                raise e
        
        # ë¡œë³´í‹±ìŠ¤ íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸
        robotics_prompt = "Decompose this task: Pick up the red cup"
        logger.info(f"ğŸ¤– ë¡œë³´í‹±ìŠ¤ í”„ë¡¬í”„íŠ¸: {robotics_prompt}")
        
        inputs = tokenizer(
            robotics_prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        try:
            with torch.no_grad():
                inputs = {k: v.to('cpu') for k, v in inputs.items()}
                
                outputs = model.generate(
                    inputs["input_ids"],
                    max_new_tokens=100,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=False
                )
            
            robotics_response = tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
            
            logger.info(f"âœ… ë¡œë³´í‹±ìŠ¤ ì‘ë‹µ: {robotics_response}")
            
        except AttributeError as e:
            if "seen_tokens" in str(e):
                logger.warning("âš ï¸ ë¡œë³´í‹±ìŠ¤ íƒœìŠ¤í¬ì—ì„œë„ DynamicCache ë¬¸ì œ, ëŒ€ì•ˆ ë°©ì‹ ì‚¬ìš©")
                
                with torch.no_grad():
                    inputs = {k: v.to('cpu') for k, v in inputs.items()}
                    outputs = model(**inputs)
                    logits = outputs.logits
                    
                    # ì—¬ëŸ¬ í† í° ìƒì„± ì‹œë®¬ë ˆì´ì…˜
                    next_tokens = []
                    for i in range(10):
                        next_token = torch.argmax(logits[0, -1, :])
                        next_tokens.append(next_token.item())
                        # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ ëœë¤ í† í° ì¶”ê°€
                        if i < 5:
                            next_tokens.append(torch.randint(0, 1000, (1,)).item())
                    
                    robotics_response = tokenizer.decode(next_tokens, skip_special_tokens=True)
                    logger.info(f"âœ… ëŒ€ì•ˆ ë°©ì‹ ë¡œë³´í‹±ìŠ¤ ì‘ë‹µ: {robotics_response}")
            else:
                raise e
        
        logger.info("ğŸ‰ PHI-3.5 ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_phi35_simple())
