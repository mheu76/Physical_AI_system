"""
PHI-3.5 Foundation Model ê³ ê¸‰ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‹¤ì œ PHI-3.5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Foundation Modelì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedFoundationModel:
    """ê³ ê¸‰ Foundation Model í´ë˜ìŠ¤ (PHI-3.5 ì‹œë„)"""
    
    def __init__(self):
        self.model_loaded = False
        self.phi35_available = False
        self.performance_metrics = {
            "missions_processed": 0,
            "successful_decompositions": 0,
            "average_response_time": 0.0,
            "phi35_usage_count": 0
        }
    
    async def initialize(self):
        """ëª¨ë¸ ì´ˆê¸°í™” - PHI-3.5 ì‹œë„"""
        logger.info("ğŸ§  Advanced Foundation Model ì´ˆê¸°í™” ì¤‘...")
        
        # PHI-3.5 ëª¨ë¸ ë¡œë”© ì‹œë„
        try:
            # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
            import transformers
            logger.info(f"âœ… Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°œê²¬: {transformers.__version__}")
            
            # PHI-3.5 ëª¨ë¸ ë¡œë”© ì‹œë„
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            model_name = "microsoft/Phi-3.5-mini-instruct"
            logger.info(f"ğŸ”„ PHI-3.5 ëª¨ë¸ ë¡œë”© ì‹œë„: {model_name}")
            
            # ë””ë°”ì´ìŠ¤ í™•ì¸
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë”© (CPU ëª¨ë“œë¡œ ì‹œë„)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            
            self.tokenizer = tokenizer
            self.model = model
            self.device = device
            self.phi35_available = True
            
            logger.info("âœ… PHI-3.5 ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
            
        except Exception as e:
            logger.warning(f"âš ï¸ PHI-3.5 ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            logger.info("ğŸ“ í´ë°± ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤ (ì‹œë®¬ë ˆì´ì…˜)")
            self.phi35_available = False
        
        self.model_loaded = True
        logger.info("âœ… Advanced Foundation Model ì´ˆê¸°í™” ì™„ë£Œ")
        return True
    
    async def _generate_with_phi35(self, prompt: str) -> str:
        """PHI-3.5ë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.phi35_available:
            return "PHI-3.5 ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            import torch  # torch ì„í¬íŠ¸ ì¶”ê°€
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"""You are a robotics task planner. Decompose this mission into subtasks:

Mission: {prompt}

Provide a structured response with subtasks in JSON format:
{{
  "subtasks": [
    {{
      "type": "navigation|manipulation|perception",
      "action": "specific_action",
      "target": "target_description",
      "priority": 1,
      "estimated_duration": 10.0,
      "difficulty": 1-5
    }}
  ]
}}

Response:"""

            # í† í°í™”
            inputs = self.tokenizer(
                full_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=1024
            )
            
            # ëª¨ë¸ì„ CPUë¡œ ì´ë™ (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
            self.model = self.model.to('cpu')
            inputs = {k: v.to('cpu') for k, v in inputs.items()}
            
            # ê°„ë‹¨í•œ ìƒì„± ë°©ì‹ ì‚¬ìš©
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs["input_ids"],
                    max_new_tokens=256,  # ë” ì§§ì€ ì¶œë ¥
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,  # ìºì‹œ ì‚¬ìš© ëª…ì‹œ
                    repetition_penalty=1.1  # ë°˜ë³µ ë°©ì§€
                )
            
            # ë””ì½”ë”©
            generated_text = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
            
            self.performance_metrics["phi35_usage_count"] += 1
            return generated_text.strip()
            
        except Exception as e:
            logger.error(f"PHI-3.5 ìƒì„± ì‹¤íŒ¨: {e}")
            return f"PHI-3.5 ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    async def interpret_mission(self, mission: str):
        """ë¯¸ì…˜ í•´ì„ - PHI-3.5 ë˜ëŠ” í´ë°± ì‚¬ìš©"""
        logger.info(f"ë¯¸ì…˜ í•´ì„ ì¤‘: {mission}")
        
        if self.phi35_available:
            # PHI-3.5 ì‚¬ìš©
            logger.info("ğŸ¤– PHI-3.5 ëª¨ë¸ ì‚¬ìš©")
            phi35_response = await self._generate_with_phi35(mission)
            logger.info(f"PHI-3.5 ì‘ë‹µ: {phi35_response[:200]}...")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                # JSON ë¶€ë¶„ ì¶”ì¶œ
                import re
                json_match = re.search(r'\{.*\}', phi35_response, re.DOTALL)
                if json_match:
                    parsed_response = json.loads(json_match.group())
                    subtasks = parsed_response.get("subtasks", [])
                    logger.info(f"âœ… PHI-3.5 JSON íŒŒì‹± ì„±ê³µ: {len(subtasks)}ê°œ ì„œë¸ŒíƒœìŠ¤í¬")
                else:
                    raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except Exception as e:
                logger.warning(f"PHI-3.5 JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                # í´ë°±ìœ¼ë¡œ ì „í™˜
                subtasks = self._fallback_mission_decomposition(mission)
        else:
            # í´ë°± ëª¨ë“œ
            logger.info("ğŸ“ í´ë°± ëª¨ë“œ ì‚¬ìš©")
            subtasks = self._fallback_mission_decomposition(mission)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.performance_metrics["missions_processed"] += 1
        self.performance_metrics["successful_decompositions"] += 1
        
        logger.info(f"âœ… ë¯¸ì…˜ í•´ì„ ì™„ë£Œ: {len(subtasks)}ê°œ ì„œë¸ŒíƒœìŠ¤í¬")
        
        return {
            "mission": mission,
            "subtasks": subtasks,
            "total_duration": sum(task.get("estimated_duration", 10.0) for task in subtasks),
            "difficulty": max(task.get("difficulty", 1) for task in subtasks) if subtasks else 1,
            "phi35_used": self.phi35_available
        }
    
    def _fallback_mission_decomposition(self, mission: str) -> List[Dict[str, Any]]:
        """í´ë°± ë¯¸ì…˜ ë¶„í•´"""
        mission_lower = mission.lower()
        
        if "pick" in mission_lower and "place" in mission_lower:
            return [
                {
                    "type": "navigation",
                    "action": "move_to",
                    "target": "object_location",
                    "priority": 1,
                    "estimated_duration": 10.0,
                    "difficulty": 2
                },
                {
                    "type": "manipulation",
                    "action": "grasp",
                    "target": "object",
                    "priority": 2,
                    "estimated_duration": 5.0,
                    "difficulty": 3
                },
                {
                    "type": "navigation",
                    "action": "move_to",
                    "target": "destination",
                    "priority": 3,
                    "estimated_duration": 10.0,
                    "difficulty": 2
                },
                {
                    "type": "manipulation",
                    "action": "place",
                    "target": "surface",
                    "priority": 4,
                    "estimated_duration": 3.0,
                    "difficulty": 2
                }
            ]
        else:
            return [
                {
                    "type": "exploration",
                    "action": "explore_environment",
                    "target": "workspace",
                    "priority": 1,
                    "estimated_duration": 30.0,
                    "difficulty": 2
                }
            ]
    
    async def process_mission_with_learning(self, mission: str, context: Dict = None):
        """í•™ìŠµì´ í¬í•¨ëœ ë¯¸ì…˜ ì²˜ë¦¬"""
        logger.info(f"í•™ìŠµ í¬í•¨ ë¯¸ì…˜ ì²˜ë¦¬: {mission}")
        
        # ë¯¸ì…˜ í•´ì„
        result = await self.interpret_mission(mission)
        
        # í•™ìŠµ ê°€ì¹˜ ê³„ì‚°
        base_learning = 0.1
        subtask_learning = len(result["subtasks"]) * 0.05
        phi35_bonus = 0.2 if result.get("phi35_used", False) else 0.0
        learning_value = base_learning + subtask_learning + phi35_bonus
        
        return {
            "success": True,
            "mission": mission,
            "result": result,
            "learning_value": learning_value,
            "performance_metrics": self.performance_metrics
        }
    
    def get_performance_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        return self.performance_metrics

async def test_advanced_foundation():
    """ê³ ê¸‰ Foundation Model í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸš€ Advanced LLM Foundation Model í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # Foundation Model ì´ˆê¸°í™”
    foundation = AdvancedFoundationModel()
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        await foundation.initialize()
        
        # í…ŒìŠ¤íŠ¸ ë¯¸ì…˜ë“¤
        test_missions = [
            "Pick up the red cup and place it on the table",
            "Move to position [1, 0, 0.5] and explore the area",
            "Organize the books on the shelf by size",
            "Clean up the workspace by putting all tools in the toolbox"
        ]
        
        # ê° ë¯¸ì…˜ í…ŒìŠ¤íŠ¸
        for i, mission in enumerate(test_missions, 1):
            logger.info(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ë¯¸ì…˜ {i}: {mission}")
            
            # ë¯¸ì…˜ í•´ì„ í…ŒìŠ¤íŠ¸
            interpretation = await foundation.interpret_mission(mission)
            logger.info(f"   í•´ì„ ê²°ê³¼: {len(interpretation['subtasks'])}ê°œ ì„œë¸ŒíƒœìŠ¤í¬")
            logger.info(f"   ì˜ˆìƒ ì‹œê°„: {interpretation['total_duration']:.1f}ì´ˆ")
            logger.info(f"   ë‚œì´ë„: {interpretation['difficulty']}/5")
            logger.info(f"   PHI-3.5 ì‚¬ìš©: {'âœ…' if interpretation.get('phi35_used', False) else 'âŒ'}")
            
            # í•™ìŠµ í¬í•¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            learning_result = await foundation.process_mission_with_learning(mission)
            logger.info(f"   í•™ìŠµ ê°€ì¹˜: {learning_result['learning_value']:.3f}")
            
            # ì„œë¸ŒíƒœìŠ¤í¬ ìƒì„¸ ì •ë³´
            for j, subtask in enumerate(interpretation['subtasks'], 1):
                logger.info(f"     {j}. {subtask['action']} -> {subtask['target']} "
                          f"({subtask['estimated_duration']:.1f}ì´ˆ, ë‚œì´ë„: {subtask['difficulty']})")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶œë ¥
        metrics = foundation.get_performance_metrics()
        logger.info(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        logger.info(f"   ì²˜ë¦¬ëœ ë¯¸ì…˜: {metrics['missions_processed']}ê°œ")
        logger.info(f"   ì„±ê³µì  ë¶„í•´: {metrics['successful_decompositions']}ê°œ")
        logger.info(f"   ì„±ê³µë¥ : {metrics['successful_decompositions']/metrics['missions_processed']*100:.1f}%")
        logger.info(f"   PHI-3.5 ì‚¬ìš© íšŸìˆ˜: {metrics['phi35_usage_count']}íšŒ")
        
        logger.info("\nâœ… Advanced LLM Foundation Model í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(test_advanced_foundation())
