#!/usr/bin/env python3
"""
DynamicCache ì˜¤ë¥˜ ìˆ˜ì • í…ŒìŠ¤íŠ¸
ê°„ë‹¨í•œ PHI-3.5 í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
"""

import asyncio
import logging
from foundation_model.phi35_integration import create_phi35_physical_ai

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_cache_fix():
    """ìºì‹œ ìˆ˜ì • í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ”§ DynamicCache ìˆ˜ì • í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # PHI-3.5 ìƒì„±
        phi35_ai = create_phi35_physical_ai(
            model_name="microsoft/Phi-3.5-mini-instruct",
            device="auto"
        )
        
        # ì´ˆê¸°í™”
        logger.info("ğŸš€ PHI-3.5 ì´ˆê¸°í™” ì¤‘...")
        success = await phi35_ai.initialize()
        
        if not success:
            logger.error("âŒ PHI-3.5 ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False
        
        logger.info("âœ… PHI-3.5 ì´ˆê¸°í™” ì„±ê³µ")
        
        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        test_prompts = [
            "Hello, how are you?",
            "What is 2+2?",
            "Tell me about robotics"
        ]
        
        for i, prompt in enumerate(test_prompts):
            logger.info(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ {i+1}: '{prompt}'")
            
            try:
                response = await phi35_ai.model_manager.generate_response(
                    prompt, 
                    max_new_tokens=50,
                    temperature=0.7
                )
                
                logger.info(f"âœ… ì‘ë‹µ ìƒì„± ì„±ê³µ:")
                logger.info(f"   ì‘ë‹µ: {response[:100]}...")
                
            except Exception as e:
                logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
                
        logger.info("\nğŸ‰ DynamicCache ìˆ˜ì • í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    asyncio.run(test_cache_fix())